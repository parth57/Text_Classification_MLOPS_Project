-------------------------Setting up project structure---------------------------

1. Create repo, clone it in local
2. Create a virtual environment named 'atlas' - conda create -n atlas python=3.10
3. Activate the virtual environment - conda activate atlas
4. pip install cookiecutter
5. cookiecutter -c v1 https://github.com/drivendata/cookiecutter-data-science 
    (This command generates a new project skeleton, -c: checkout, v1: branch/git tag name)
6. Rename src.models -> src.model
7. git add - commit - push

-------------------------Setup MLFlow on Dagshub---------------------------
8. Go to: https://dagshub.com/dashboard
9. Create > New Repo > Connect a repo > (Github) Connect > Select your repo > Connect
10. Copy experiment tracking url and code snippet. (Also try: Go To MLFlow UI)
11. pip install dagshub & mlflow

12. Run the exp notebooks
    • Iteration 1: This is about sentiment analysis (positive/negative) of IMDB movie data
    • First a simple baseline model was used which is logistic regression
        • Preprocessing Steps:
            • lower_case, remove_stop_words, removing_numbers, removing_punctuations, removing_urls, lemmatization
            • The data was sampled i.e. 500 samples out of more than 50k records and was fairly balanced
            • The target was encoded as 0/1
            • Missing values check
            • CountVectorizer
            • train-test split
        • Set Remote Tracking URI, mlflow experiments will be hosted on Dagshub server.
        • Then initializing dagshub with the configuration i.e. repo owner and repo name and setting mlflow = True
        • setting experiment name in mlflow
    • Tracking the model run with mlflow by logging model params like max features vectorized, train-test split ratio,
        tracking model metrics and model itself.

    • Iteration 2: Here we have tried with multiple algortithms like LogisticRegression, MultinomialNB, XGBoost, RF, GradientBoosting
        and trying out differenct vectorizing techniques like CountVectorizer, TF-IDF and Word2Vec
    • All these combinations were compared in mlflow ui, and the best algorithm with the best vectorizing technique was selected 
        which was Logistic Regression with tf-idf.
    
    • Iteration 3: It's about hyperparameter tuning of Logistic Regression with 5 fold cv and scoring metrics as 'f1' 
        and applying Grid Search CV and then again tracking the best hyperparameter, metrics in mlflow ui and then selecting that 
            for the developement at the current stage.

13. git add - commit - push

14. dvc init
15. create a local folder as "local_s3" (temporary work)
16. on terminal: "dvc remote add -d mylocal local_s3" 
    (It adds a new DVC remote storage location named mylocal, points it to local_s3, and sets it as the default (-d) remote for the
     project.)

17. Add code to below files/folders inside src dir:
    - logger
    - data_ingestion.py
    - data_preprocessing.py
    - feature_engineering.py
    - model_building.py
    - model_evaluation.py (This is obviosuly done on the best hyperparameters and best vectorization/algorithms selected)
    - register_model.py
        • This registers the model in mlfow and is transitioned to a wanted state i.e. PROD/STG using MLFlow Client 
        • MLFLow requires a uri built on run id and model path
18. add file - dvc.yaml (till model evaluation.metrics)
19. add file - params.yaml
20. DVC pipeline is ready to run - dvc repro
21. Once do - dvc status
22. git add - commit - push

23. Need to add S3 as remote storage - Create IAM User(keep cred) and S3 bucket
24. pip install - dvc[s3] & awscli
25. Checking/deleting dvc remote (optional) - [dvc remote list & dvc remote remove <name>] 
26. Set aws cred - aws configure
27. Add s3 as dvc remote storage - dvc remote add -d myremote s3://<bucket-name>    (After this dvc commit and push) 
    • This will have all the data versions of input/output, intermediate artifacts (params files) and model params in the form of 
        data BLOB i.e. (Binary Large Object Data Type used for storing large chunks of binary files)
        addressed by hash i.e. md5 (Message-Digest Algorithm 5, which is a 128-bit output generated by a
        hash function that takes any input data and creates a unique, fixed-length "fingerprint" (a 32-character hexadecimal string))

28. Create new dir - flask_app | Inside that, add rest of the files and dir, app.py file has:
    • Mlflow Client first sets the connection/communication with dagshub remote server and then initiates the dagshub
    • It creates a custom registry in Promotheus apart from global registry which it has already
        • In this registry, we track the request count, latency i.e. how long a particular request took to be handled by the application
    • Befofe the second point, it first loads the model with the latest version from mlflow registry from Staging and loads
      vectorizer from the location it was dumped (Not from mlflow)
    • It then sets the routes
        • '/' for accessing the application on browser, request made by user, then it increments the request counter and tracks latency
        • '/predict' for making the prediction using the input text by user, inside this API, the text is cleaned and transformed into
            features so that model can predict it, then finally renders the results to index.html page
        • '/metrics' for returning the custom metrics defined by you along with the global metrics by Promotheus
29. pip install flask and run the app (dvc push - to push data to S3)

30. pip freeze > requirements.txt
31. Add .github/workflows/ci.yaml file

32. Create key token on Dagshub for auth: Go to dagshub repo > Your settings > Tokens > Generate new token
    >> Please make sure to save token << >> capstone_test: 54b1d67648a9b1267ef906fsdfsd8b292f779f0<<
    >> Add this auth token to github secret&var and update on ci file


31. Add dir "tests"&"scripts" and files within. This will contain our test related scripts for CI.
