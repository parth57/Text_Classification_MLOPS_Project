-------------------------Setting up project structure---------------------------

1. Create repo, clone it in local
2. Create a virtual environment named 'atlas' - conda create -n atlas python=3.10
3. Activate the virtual environment - conda activate atlas
4. pip install cookiecutter
5. cookiecutter -c v1 https://github.com/drivendata/cookiecutter-data-science 
    (This command generates a new project skeleton, -c: checkout, v1: branch/git tag name)
6. Rename src.models -> src.model
7. git add - commit - push

-------------------------Setup MLFlow on Dagshub---------------------------
8. Go to: https://dagshub.com/dashboard
9. Create > New Repo > Connect a repo > (Github) Connect > Select your repo > Connect
10. Copy experiment tracking url and code snippet. (Also try: Go To MLFlow UI)
11. pip install dagshub & mlflow

12. Run the exp notebooks
    • Iteration 1: This is about sentiment analysis (positive/negative) of IMDB movie data
    • First a simple baseline model was used which is logistic regression
        • Preprocessing Steps:
            • lower_case, remove_stop_words, removing_numbers, removing_punctuations, removing_urls, lemmatization
            • The data was sampled i.e. 500 samples out of more than 50k records and was fairly balanced
            • The target was encoded as 0/1
            • Missing values check
            • CountVectorizer
            • train-test split
        • Set Remote Tracking URI, mlflow experiments will be hosted on Dagshub server.
        • Then initializing dagshub with the configuration i.e. repo owner and repo name and setting mlflow = True
        • setting experiment name in mlflow
    • Tracking the model run with mlflow by logging model params like max features vectorized, train-test split ratio,
        tracking model metrics and model itself.

    • Iteration 2: Here we have tried with multiple algortithms like LogisticRegression, MultinomialNB, XGBoost, RF, GradientBoosting
        and trying out differenct vectorizing techniques like CountVectorizer, TF-IDF and Word2Vec
    • All these combinations were compared in mlflow ui, and the best algorithm with the best vectorizing technique was selected 
        which was Logistic Regression with tf-idf.
    
    • Iteration 3: It's about hyperparameter tuning of Logistic Regression with 5 fold cv and scoring metrics as 'f1' 
        and applying Grid Search CV and then again tracking the best hyperparameter, metrics in mlflow ui and then selecting that 
            for the developement at the current stage.

13. git add - commit - push

14. dvc init
15. create a local folder as "local_s3" (temporary work)
16. on terminal: "dvc remote add -d mylocal local_s3" 
    (It adds a new DVC remote storage location named mylocal, points it to local_s3, and sets it as the default (-d) remote for the
     project.)

17. Add code to below files/folders inside src dir:
    - logger
    - data_ingestion.py
    - data_preprocessing.py
    - feature_engineering.py
    - model_building.py
    - model_evaluation.py (This is obviosuly done on the best hyperparameters and best vectorization/algorithms selected)
    - register_model.py
        • This registers the model in mlfow and is transitioned to a wanted state i.e. PROD/STG using MLFlow Client 
        • MLFLow requires a uri built on run id and model path
18. add file - dvc.yaml (till model evaluation.metrics)
19. add file - params.yaml
20. DVC pipeline is ready to run - dvc repro
21. Once do - dvc status
22. git add - commit - push

23. Need to add S3 as remote storage - Create IAM User(keep cred) and S3 bucket
24. pip install - dvc[s3] & awscli
25. Checking/deleting dvc remote (optional) - [dvc remote list & dvc remote remove <name>] 
26. Set aws cred - aws configure
27. Add s3 as dvc remote storage - dvc remote add -d myremote s3://<bucket-name>    (After this dvc commit and push) 
    • This will have all the data versions of input/output, intermediate artifacts (params files) and model params in the form of 
        data BLOB i.e. (Binary Large Object Data Type used for storing large chunks of binary files)
        addressed by hash i.e. md5 (Message-Digest Algorithm 5, which is a 128-bit output generated by a
        hash function that takes any input data and creates a unique, fixed-length "fingerprint" (a 32-character hexadecimal string))

28. Create new dir - flask_app | Inside that, add rest of the files and dir, app.py file has:
    • Mlflow Client first sets the connection/communication with dagshub remote server and then initiates the dagshub
    • It creates a custom registry in Promotheus apart from global registry which it has already
        • In this registry, we track the request count, latency i.e. how long a particular request took to be handled by the application
    • Befofe the second point, it first loads the model with the latest version from mlflow registry from Staging and loads
      vectorizer from the location it was dumped (Not from mlflow)
    • It then sets the routes
        • '/' for accessing the application on browser, request made by user, then it increments the request counter and tracks latency
        • '/predict' for making the prediction using the input text by user, inside this API, the text is cleaned and transformed into
            features so that model can predict it, then finally renders the results to index.html page
        • '/metrics' for returning the custom metrics defined by you along with the global metrics by Promotheus
29. pip install flask and run the app (dvc push - to push data to S3)

30. pip freeze > requirements.txt
31. Add .github/workflows/ci.yaml file: This contains all the pipline steps of CI, eventually it runs the dvc rero to run the pipeline


32. Create key token on Dagshub for auth: Go to dagshub repo > Your settings > Tokens > Generate new token
    >> Please make sure to save token << >> capstone_test: 54b1d67648a9b1267ef906fsdfsd8b292f779f0<<
    >> Add this auth token to github secret&var and update on ci file


31. Add dir "tests"&"scripts" and files within. This will contain our test related scripts for CI.
    • In scripts dir, promote_mode.py first archive any model in production in mlflow - > Then fetches the latest version of model
        in staging and then promotes to the production env. This is all done using mlfow's in-built functions. Needless to mention
        only the accepted model after experimenting will be pushed to staging env.
    • Inside tests dir, test_flask_app.py first tests the home route by checking if the response code after hitting the API is 200 or not.
      Also, it checks the predict route-> it checks whether response code after hitting the predict button is 200 or not, also it checks
        whether the response is either equal to Positive/Negative.
    • Inside tests dir, test_model.py first sets up DagsHub credentials for MLflow tracking, then inside your mlflow model registry
        loads the latest verison of the model - > Loads the vectorizer.pkl and the test data present in data/processed.
        After this it runs a few test before this model is pushed to staging env in mlflow (it has to exceed some accuracy etc thresholds)
            • Create a dummy input for the model based on expected input shape
            • Predict using the new model to verify the input and output shapes
                • Verify the input shape
                • Verify the output shape (assuming binary classification with a single output)
            • Test Model Performance
                • Calculate performance metrics for the new model (Acuracy, F1 Score, Precision, Recall)
                • Define expected thresholds for the performance metrics (Can be changed based on the current state of performance of your model)
                • The current metrics score should be greater than the thresholds defined above, only then it would be pushed to staging in mlflow
        Note: All these unit tests are done using the unittest library
    • Now if you try to run the app.py locally, it will fetch the model in production from mlflow model registry to do the prediction
        and you will see the output in /predict route after submitting your input text in your application UI.

>>>>> Moving to Docker <<<<<
32. pip install pipreqs (This is to create a requirements.txt file)
33. cd flask_app & do "pipreqs . --force" (This creates the requirements.txt file and overwrites if there's an existing one using --force)
34. Add dockerfile and start docker-desktop in background
    Also before proceeding make sure: [switch the mlflow server setup to param version (user defined mlfow uri and dagshub config), 
    change cmd on dockerfile to run it locally]
35. go to root dir and: "docker build -t text_class:latest ."
36. Try running the image: "docker run -p 8888:5000 text_class:latest"
    - This run will give 'OSError: capstone_test environment variable is not set'...obviously
    - alternate: docker run -p 8888:5002 -e CAPSTONE_TEST='f39dbdc19cb926e11eca43825499eb7cb43d25bd' text_class:latest
        • For this just make sure you are using the prod params in the "model_evaluation, model register_model and app script"
        • '-e' sets the environment varibale during run time
    - docker push your_user/text_class:latest (optional)
    - optional: try to delete image locally and pull it from dockerhub and run (optional)

37. Setup aws services for below secrets and variables: Using "aws configure" command
	AWS_ACCESS_KEY_ID
	AWS_SECRET_ACCESS_KEY
	AWS_REGION
	ECR_REPOSITORY (name: sentiment_analysis)
    AWS_ACCOUNT_ID (396608771834)
   (Also add this permission to the IAM user: AmazonEC2ContainerRegistryFullAccess)

   Once this is done, add these credentials in Github Secret Variables: Go to settings->Secrets and variables->Actions

38. Execute CICD pipeline till the stage where we build and push image to ECR
    • This is done to push the docker image built to ECR
    • We faced an error while logging into Amazon ECR beacuse of a conflicting policy which was causing this issue i.e.
        denying the access. Hence we deleted this policy and again created aws secred and aws secret access key. Post this
        we updated the same in github secrets variables. After this CICD Pipeline ran as expected.

----------------------------------------------------------------------------------
*********Setup required before moving to EKS deployment*********
----------------------------------------------------------------------------------
* Verify AWS CLI: aws --version
* Verify kubectl: kubectl version --client
* Verify eksctl: eksctl version
----------------------------------------------------------------------------------

39. Create an EKS cluster:
    eksctl create cluster --name flask-app-cluster --region us-east-1 --nodegroup-name flask-app-nodes --node-type t3.small --nodes 1 --nodes-min 1 --nodes-max 1 --managed

    command understanding: This creates an EKS cluster with name "flask-app-cluster"->specify region->specify nodegroup named 
                            "flask-app-nodes" -> specify node type (EC2 servers) present in AWS (eg: t3 small based on size of image)
                            -> ask for total nodes->specify min/max nodes -> 
                            managed: AWS handles the load balancing or handles your eks cluster

    Note: eksctl creates and manages the EKS infrastructure, while kubectl operates the Kubernetes cluster that runs on that
             infrastructure. Eksctl mnaages EKS clusters, Node groups (EC2 / Fargate), IAM roles & policies, Cluster upgrades.
             kubectl is the standard Kubernetes client which manages Pods, Deployments, Deployments, Scaling & rollouts

    • AWS Security Token Service (AWS STS) is a web service that allows you to request temporary, limited-privilege security 
        credentials for users or applications within your AWS environment

40. Update kubectl Config(Once the cluster is created, eksctl will automatically update your kubectl config file. However, you can verify and set it manually using:)
aws eks --region us-east-1 update-kubeconfig --name flask-app-cluster (This ensures your kubectl is pointing to the correct cluster.)

41. Check EKS Cluster Configuration Ensure you can access your EKS cluster by running
    aws eks list-clusters

42. Delete cluster(optional):
    eksctl delete cluster --name flask-app-cluster --region us-east-1

    Also, verify cluster deletion:
    eksctl get cluster --region us-east-1

43. Verify the cluster status:
    aws eks --region us-east-1 describe-cluster --name flask-app-cluster --query "cluster.status"


44. Check cluster connectivity:
kubectl get nodes

45. Check the namespaces:
kubectl get namespaces
 
46. Verify the deployment:
kubectl get pods
kubectl get svc (svc: service)

47. Deploy the app on EKS via CICD pipeline 
  >> edit ci.yaml, deployment.yaml, dockerfile
  >> Also edit the security group for nodes and edit inbound rule for 5002 port (By going to EC2 server set by eksctl)
  >> Push it to git for CI-CD Pipeline
  >> Once successfully done: kubectl get pods (Look for Status) -> kubectl describe pod <pod-name>

48. Once the LoadBalancer service is up, get the external IP:
kubectl get svc flask-app-service

49. Try externa-ip:5000 directly on url or on terminal : curl http://external-ip:5002 (external-ip: obtained in above step followed by :5002)
curl http://a983b596774a64558983d69e869fb92a-1267822012.us-east-1.elb.amazonaws.com:5002




>>>>>>>>>> Prometheus Server Setup <<<<<<<<<<

50. Launch an Ubuntu EC2 Instance for Prometheus: t3.medium,  20GB of disk space (general-purpose SSD),
     Security Group: Allow inbound access on ports: 9090 for Prometheus Web UI, 22 for SSH access

51. SSH into the EC2 Instance(optional or connect directly to ec2 server alternatively):
ssh -i your-key.pem ubuntu@your-ec2-public-ip

52. Update packages: sudo apt update && sudo apt upgrade -y

53. Download Prometheus:
wget https://github.com/prometheus/prometheus/releases/download/v2.46.0/prometheus-2.46.0.linux-amd64.tar.gz
tar -xvzf prometheus-2.46.0.linux-amd64.tar.gz
mv prometheus-2.46.0.linux-amd64 prometheus

54. Move files to standard paths:
sudo mv prometheus /etc/prometheus
sudo mv /etc/prometheus/prometheus /usr/local/bin/

55. Create Prometheus Configuration: 
>> Open the file for editing: sudo nano /etc/prometheus/prometheus.yml
>> Edit the File:

global:
  scrape_interval: 15s

(Add the following in prometheus.yml with correct indentation)
scrape_configs:
  - job_name: "flask-app"
    static_configs:
      - targets: ["a983b596774a64558983d69e869fb92a-1267822012.us-east-1.elb.amazonaws.com:5002"]  # Replace with your app's External IP



>> Save the File: ctrl+o -> enter -> ctrl+x
>> Verify the Changes: cat /etc/prometheus/prometheus.yml

56. Locate the Prometheus Binary(Run the following command to find where the prometheus executable is installed):
which prometheus
This should return the full path to the prometheus binary, such as /usr/local/bin/Prometheus

57. Run Prometheus with the config file:
/usr/local/bin/prometheus --config.file=/etc/prometheus/prometheus.yml

Note: For checking what prometheus is monitoring: http://a983b596774a64558983d69e869fb92a-1267822012.us-east-1.elb.amazonaws.com:5002/metrics

>>>>>>>>>> Grafana Server Setup <<<<<<<<<<

58. Launch an Ubuntu EC2 Instance for Grafana: t3.medium,  20GB of disk space (general-purpose SSD), Security Group: Allow inbound access on ports: 3000 for Grafana Web UI, 22 for SSH access

59. SSH into the EC2 Instance(optional or connect directly to ec2 server alternatively):
ssh -i your-key.pem ubuntu@your-ec2-public-ip

60. Update and upgrade system packages:
sudo apt update && sudo apt upgrade -y

61. Download Grafana: wget https://dl.grafana.com/oss/release/grafana_12.3.1_amd64.deb
(this is a stable version for now; adjust the link if necessary.)

62. Install Grafana: sudo apt install ./grafana_12.3.1_amd64.deb -y
    Note:  After this step: Security Group: Allow inbound access on ports: 3000 for Grafana Web UI

63. Start the Grafana service: sudo systemctl start grafana-server

64. Enable Grafana to start on boot: sudo systemctl enable grafana-server

65. Verify the service is running: sudo systemctl status grafana-server

66. Open Grafana web UI: http://<ec2-public-ip>:3000 (username/pass - admin)

67. Add Prometheus as a Data Source: http://44.201.60.162:9090
    click - Save and Test | Get started with building dashboards.





----------------------------------------------------------------------------------

AWS Resource Cleanup:

* Delete deployment - kubectl delete deployment flask-app
* Delete service - kubectl delete service flask-app-service
* Delete env var - kubectl delete secret capstone-secret
* Delete EKS Cluster - eksctl delete cluster --name flask-app-cluster --region us-east-1
* Verify Cluster Deletion - eksctl get cluster --region us-east-1
* Delete artifacts of ECR and S3 (optional - delete ECR and S3)
* Validate if Cloud Formation stacks are deleted.
* Confirm service termination on AWS support chat.


----------------------------------------------------------------------------------


***** How Is CloudFormation Related to EKS? *****

What Is CloudFormation? 
AWS CloudFormation is a service that helps define and provision AWS infrastructure as code using templates. It automates the process of creating and managing AWS resources.

How Does It Relate to EKS?
When you run eksctl, it generates CloudFormation templates behind the scenes to create:
1. The EKS control plane.
2. Node groups.
CloudFormation ensures that these resources are created and managed as a stack (a logical grouping of resources).

What Is a Stack in CloudFormation?
A CloudFormation Stack is a collection of AWS resources (like VPCs, subnets, EC2 instances, etc.) managed as a single unit. For EKS:
1. eksctl-flask-app-cluster-cluster stack: Creates the EKS control plane.
2. eksctl-flask-app-cluster-nodegroup-flask-app-nodes stack: Creates the worker nodes.

Each stack contains:

1. A template defining the resources (YAML or JSON).
2. Resource dependencies and configurations.

----------------------------------------------------------------------------------

----------------------------------------------------------------------------------

Fleet Requests
Definition: A Fleet Request is an AWS internal process for provisioning EC2 instances. When creating a NodeGroup, EKS uses an Auto Scaling Group (ASG) to request EC2 instances, which count as Fleet Requests.

Why Relevant?: AWS imposes a limit on the number of Fleet Requests per account. If your account exceeds this limit (e.g., due to other active ASGs or EKS clusters), NodeGroup creation fails with the error "You’ve reached your quota for maximum Fleet Requests".
----------------------------------------------------------------------------------

----------------------------------------------------------------------------------

What is a PVC?
1. PVC (PersistentVolumeClaim) is a request for storage by a Kubernetes application. It is bound to a PV (PersistentVolume), which is the actual storage resource.
2. PVCs use StorageClasses to define how the volume should be provisioned (e.g., EBS volumes on AWS, NFS, etc.).
3. When a pod is deployed and needs storage, it will request the storage defined in the PVC.

----------------------------------------------------------------------------------